{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tabulate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# datasets\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "\n",
    "# transformers\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import IntervalStrategy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_using_tabulate(data):\n",
    "    table_data = []\n",
    "    for key, values in data.items():\n",
    "        if key != 'macro avg' and key != 'weighted avg':\n",
    "            if isinstance(values, dict):\n",
    "                row = [key, values['precision'], values['recall'], values['f1-score'], values['support']]\n",
    "                table_data.append(row)\n",
    "\n",
    "    # Print the classification report using tabulate\n",
    "    headers = ['Class', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
    "    m_table = tabulate(table_data, headers=headers, tablefmt='psql',floatfmt=\".4f\")\n",
    "    print(m_table)\n",
    "\n",
    "\n",
    "def plotConfusionMatrix(title, confusionMat, targetNames):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    ax.matshow(confusionMat, aspect='auto', vmin=0, vmax=1000, cmap=plt.get_cmap('Blues'))\n",
    "    plt.ylabel('Predicted Category')\n",
    "    plt.yticks(range(len(targetNames)), targetNames)\n",
    "    plt.xlabel('Actual Category')\n",
    "    plt.xticks(range(len(targetNames)), targetNames, rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"train.csv\"\n",
    "test_path = \"test.csv\"\n",
    "valid_path = \"valid.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "valid_df = pd.read_csv(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['stars'].isin([1, 2, 4, 5])]\n",
    "# train_df = train_df[train_df['language'].isin(['en', 'de', 'fr', 'es'])]\n",
    "train_df = train_df[train_df['language'].isin(['en'])]\n",
    "\n",
    "test_df = test_df[test_df['stars'].isin([1, 2, 4, 5])]\n",
    "# test_df = test_df[test_df['language'].isin(['en', 'de', 'fr', 'es'])]\n",
    "test_df = test_df[test_df['language'].isin(['en'])]\n",
    "\n",
    "valid_df = valid_df[valid_df['stars'].isin([1, 2, 4, 5])]\n",
    "# test_df = test_df[test_df['language'].isin(['en', 'de', 'fr', 'es'])]\n",
    "valid_df = valid_df[valid_df['language'].isin(['en'])]\n",
    "\n",
    "\n",
    "columns_to_drop = ['review_id', 'product_id', 'reviewer_id', 'product_category']\n",
    "train_df.drop(columns=columns_to_drop, inplace=True)\n",
    "test_df.drop(columns=columns_to_drop, inplace=True)\n",
    "valid_df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_to_delete = 35000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in ['en']:\n",
    "# for lang in ['en', 'de', 'es', 'fr']:\n",
    "    for star in [1, 2, 4, 5]:\n",
    "        print(f'Language: {lang}, Stars: {star}')\n",
    "        print(f'Number of rows before: {len(train_df[(train_df[\"language\"] == lang) & (train_df[\"stars\"] == star)])}')\n",
    "        random_indices = np.random.choice(train_df[(train_df['language'] == lang) & (train_df['stars'] == star)].index, num_rows_to_delete, replace=False)\n",
    "        train_df.drop(index=random_indices, inplace=True)\n",
    "        print(f'Number of rows after: {len(train_df[(train_df[\"language\"] == lang) & (train_df[\"stars\"] == star)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mapping(df, label):\n",
    "    for i, r in df.iterrows():\n",
    "        if r[label] >= 4:\n",
    "            df.loc[i, label] = 1\n",
    "        else:\n",
    "            df.loc[i, label] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = replace_mapping(test_df, 'stars')\n",
    "valid_df = replace_mapping(valid_df, 'stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = replace_mapping(train_df, 'stars')\n",
    "test_df = replace_mapping(test_df, 'stars')\n",
    "valid_df = replace_mapping(valid_df, 'stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df[\"stars\"].unique())\n",
    "print(test_df[\"stars\"].unique())\n",
    "print(valid_df[\"stars\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_truncation(data_df, tokenizer, m, n):\n",
    "    # Create a tqdm progress bar for the loop\n",
    "    for i, r in tqdm(data_df.iterrows(), total=len(data_df), desc=\"Processing reviews\"):\n",
    "        tokenized_row = tokenizer.tokenize(r['review_body'])\n",
    "        if len(tokenized_row) > m+n:\n",
    "            data_df.loc[i, 'review_body'] = tokenizer.convert_tokens_to_string(tokenized_row[:m] + tokenized_row[-n:])\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "M = 382\n",
    "# N = 64\n",
    "# M = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Negative', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_truncation(train_df, tokenizer, M, N)\n",
    "test_df = prepare_truncation(test_df, tokenizer, M, N)\n",
    "valid_df = prepare_truncation(valid_df, tokenizer, M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "valid_ds = Dataset.from_pandas(valid_df)\n",
    "\n",
    "\n",
    "en_only_dataset = DatasetDict()\n",
    "\n",
    "en_only_dataset['train'] = train_ds\n",
    "en_only_dataset['validation'] = valid_ds\n",
    "en_only_dataset['test'] = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function with labels\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"review_body\"], truncation=True)\n",
    "    inputs[\"labels\"] = examples[\"stars\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train = en_only_dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_validation = en_only_dataset[\"validation\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = en_only_dataset[\"test\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# Data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "\n",
    "# Load metrics outside the compute_metrics function\n",
    "load_accuracy = evaluate.load(\"accuracy\")\n",
    "load_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    warmup_steps=10000,\n",
    "    optim=\"adamw_torch\",\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=1e-4,\n",
    "    save_strategy=IntervalStrategy.STEPS,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps=500,\n",
    "    logging_strategy=IntervalStrategy.STEPS,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=5,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "for f in os.listdir(\"weights\"):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f)\n",
    "    # # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(\"weights/binary/\"):\n",
    "    model_name\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f)\n",
    "    # # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
