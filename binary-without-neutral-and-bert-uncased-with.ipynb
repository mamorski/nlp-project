{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[torch]\n!pip install sentencepiece\n!pip install datasets\n!pip install evaluate","metadata":{"id":"I0Gdjq43mLI-","outputId":"a12c8764-a114-4685-85d1-e71810e9419b","execution":{"iopub.status.busy":"2023-08-10T11:41:55.932390Z","iopub.execute_input":"2023-08-10T11:41:55.932710Z","iopub.status.idle":"2023-08-10T11:42:45.089903Z","shell.execute_reply.started":"2023-08-10T11:41:55.932682Z","shell.execute_reply":"2023-08-10T11:42:45.088699Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.65.0)\nRequirement already satisfied: torch!=1.12.0,>=1.9 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\nRequirement already satisfied: accelerate>=0.20.2 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nfrom tabulate import tabulate\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report\n\n# datasets\nfrom datasets import Dataset\nfrom datasets import DatasetDict\nfrom datasets import load_metric\nfrom datasets import load_dataset\n\n# transformers\nfrom transformers import Trainer\nfrom transformers import AutoTokenizer\nfrom transformers import DataCollatorWithPadding\nfrom transformers import TrainingArguments\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import IntervalStrategy\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\n\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport evaluate","metadata":{"id":"SIyT2McYmLJA","execution":{"iopub.status.busy":"2023-08-10T12:22:03.310104Z","iopub.execute_input":"2023-08-10T12:22:03.310795Z","iopub.status.idle":"2023-08-10T12:22:03.320141Z","shell.execute_reply.started":"2023-08-10T12:22:03.310712Z","shell.execute_reply":"2023-08-10T12:22:03.319162Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:22:03.322404Z","iopub.execute_input":"2023-08-10T12:22:03.323039Z","iopub.status.idle":"2023-08-10T12:22:03.334194Z","shell.execute_reply.started":"2023-08-10T12:22:03.323003Z","shell.execute_reply":"2023-08-10T12:22:03.332930Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from datasets.utils.logging import disable_progress_bar\nfrom transformers import logging\n\n\ndisable_progress_bar()\nlogging.set_verbosity_error()","metadata":{"id":"jFp7fxqNE9xx","execution":{"iopub.status.busy":"2023-08-10T12:22:03.335643Z","iopub.execute_input":"2023-08-10T12:22:03.336316Z","iopub.status.idle":"2023-08-10T12:22:03.349243Z","shell.execute_reply.started":"2023-08-10T12:22:03.336282Z","shell.execute_reply":"2023-08-10T12:22:03.348273Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"directory = \"/kaggle/input/amazon-with-translated\"\n# directory = \"/content/drive/MyDrive/MSc/NLP/nlp-project\"","metadata":{"id":"kN_UjOK836FV","execution":{"iopub.status.busy":"2023-08-10T12:22:03.350816Z","iopub.execute_input":"2023-08-10T12:22:03.351474Z","iopub.status.idle":"2023-08-10T12:22:03.361866Z","shell.execute_reply.started":"2023-08-10T12:22:03.351440Z","shell.execute_reply":"2023-08-10T12:22:03.360899Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_path = f\"{directory}/train.csv\"\ntest_path = f\"{directory}/test.csv\"\nvalid_path = f\"{directory}/valid.csv\"\ntranslated_test_path = f\"{directory}/amazon_translated_body_and_title_with_originals_all_stars.csv\"","metadata":{"id":"xzo4NgBBmLJB","execution":{"iopub.status.busy":"2023-08-10T12:22:03.364710Z","iopub.execute_input":"2023-08-10T12:22:03.365389Z","iopub.status.idle":"2023-08-10T12:22:03.378067Z","shell.execute_reply.started":"2023-08-10T12:22:03.365355Z","shell.execute_reply":"2023-08-10T12:22:03.377129Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\nvalid_df = pd.read_csv(valid_path)\ntranslated_test_df = pd.read_csv(translated_test_path)","metadata":{"id":"_jq8AL2SmLJB","execution":{"iopub.status.busy":"2023-08-10T12:22:03.379697Z","iopub.execute_input":"2023-08-10T12:22:03.380419Z","iopub.status.idle":"2023-08-10T12:22:11.270412Z","shell.execute_reply.started":"2023-08-10T12:22:03.380384Z","shell.execute_reply":"2023-08-10T12:22:11.269345Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"translated_test_df.rename(columns={\"review_body\": \"review_body_original\", \"review_title\": \"review_title_original\"}, inplace=True)","metadata":{"id":"FLKCiae-6WxA","execution":{"iopub.status.busy":"2023-08-10T12:22:11.272254Z","iopub.execute_input":"2023-08-10T12:22:11.272668Z","iopub.status.idle":"2023-08-10T12:22:11.278762Z","shell.execute_reply.started":"2023-08-10T12:22:11.272630Z","shell.execute_reply":"2023-08-10T12:22:11.277557Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"translated_test_df.rename(columns={\"translated_body\": \"review_body\", \"translated_title\": \"review_title\"}, inplace=True)","metadata":{"id":"px5xhV2k68sc","execution":{"iopub.status.busy":"2023-08-10T12:22:11.280369Z","iopub.execute_input":"2023-08-10T12:22:11.281073Z","iopub.status.idle":"2023-08-10T12:22:11.293265Z","shell.execute_reply.started":"2023-08-10T12:22:11.281036Z","shell.execute_reply":"2023-08-10T12:22:11.292311Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n","metadata":{"id":"ljcytX1KmLJB","outputId":"c62fa29e-01d3-4922-eecf-d21d2a0b003a","execution":{"iopub.status.busy":"2023-08-10T12:22:11.294826Z","iopub.execute_input":"2023-08-10T12:22:11.295169Z","iopub.status.idle":"2023-08-10T12:22:11.308759Z","shell.execute_reply.started":"2023-08-10T12:22:11.295137Z","shell.execute_reply":"2023-08-10T12:22:11.307552Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"SEED = 111\n\n# Set the random seed for Python to SEED\nrandom.seed(SEED)\n\n# Set the random seed for numpy to SEED\nnp.random.seed(SEED)\n\n# Set the random seed for torch to SEED\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)","metadata":{"id":"eqXdSpJCmLJC","execution":{"iopub.status.busy":"2023-08-10T12:22:11.313125Z","iopub.execute_input":"2023-08-10T12:22:11.313427Z","iopub.status.idle":"2023-08-10T12:22:11.322094Z","shell.execute_reply.started":"2023-08-10T12:22:11.313402Z","shell.execute_reply":"2023-08-10T12:22:11.321085Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def verbose_print(msg, verbose=False):\n    if verbose:\n        print(msg)\n\n\ndef drop_data(df, stars, languages, columns_to_drop):\n    df = df[df[\"stars\"].isin(stars)]\n    df = df[df[\"language\"].isin(languages)]\n    if len(columns_to_drop) > 0:\n        df.drop(columns=columns_to_drop, inplace=True)\n    df.reset_index(drop=True, inplace=True)\n\n    return df\n\n\ndef prepare_data(\n    train, test, valid, columns_to_drop, stars=[1, 2, 3, 4], languages=[\"en\"]\n):\n    train = drop_data(train, stars, languages, columns_to_drop)\n    test = drop_data(test, stars, languages, columns_to_drop)\n    valid = drop_data(valid, stars, languages, columns_to_drop)\n\n    return train, test, valid\n\n\ndef reduce_dataset(df, stars, languages, num_of_rows_to_drop, verbose=False):\n    for lang in languages:\n        for star in stars:\n            verbose_print(f\"Language: {lang}, Stars: {star}\", verbose)\n            verbose_print(\n                f'Number of rows before: {len(df[(df[\"language\"] == lang) & (df[\"stars\"] == star)])}',\n                verbose,\n            )\n            random_indices = np.random.choice(\n                df[(df[\"language\"] == lang) & (df[\"stars\"] == star)].index,\n                num_of_rows_to_drop,\n                replace=False,\n            )\n            df.drop(index=random_indices, inplace=True)\n            verbose_print(\n                f'Number of rows after: {len(df[(df[\"language\"] == lang) & (df[\"stars\"] == star)])}',\n                verbose,\n            )\n\n    return df\n\n\ndef replace_mapping(df, label, mapping):\n    for k, v in mapping.items():\n        df[label].replace(k, v, inplace=True)\n    return df\n\n\ndef prepare_truncation(data_df, tokenizer, m, n):\n    for i, r in tqdm(data_df.iterrows(), total=len(data_df), desc=\"Processing reviews\"):\n        tokenized_row = tokenizer.tokenize(r[\"review_body\"])\n        if len(tokenized_row) > m + n:\n            data_df.loc[i, \"review_body\"] = tokenizer.convert_tokens_to_string(\n                tokenized_row[:m] + tokenized_row[-n:]\n            )\n\n    return data_df\n\n\ndef print_using_tabulate(data):\n    table_data = []\n    for key, values in data.items():\n        if key != \"macro avg\" and key != \"weighted avg\":\n            if isinstance(values, dict):\n                row = [\n                    key,\n                    values[\"precision\"],\n                    values[\"recall\"],\n                    values[\"f1-score\"],\n                    values[\"support\"],\n                ]\n                table_data.append(row)\n\n    # Print the classification report using tabulate\n    headers = [\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]\n    m_table = tabulate(table_data, headers=headers, tablefmt=\"psql\", floatfmt=\".4f\")\n    print(m_table)\n\n\ndef eval_model(trainer, test_set, target_names, label):\n    predictions = trainer.predict(test_set)\n    predicted_labels = predictions.predictions.argmax(axis=1)\n    ground_truth_labels = test_set[label]\n    classification_reports = classification_report(\n        ground_truth_labels,\n        predicted_labels,\n        target_names=target_names,\n        output_dict=True,\n    )\n    print_using_tabulate(classification_reports)\n    print(\"\\n\\n\\n\")\n\n\nload_accuracy = evaluate.load(\"accuracy\")\nload_f1 = evaluate.load(\"f1\")\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\n        \"accuracy\"\n    ]\n    f1 = load_f1.compute(\n        predictions=predictions, references=labels, average=\"weighted\"\n    )[\"f1\"]\n    return {\"accuracy\": accuracy, \"f1\": f1}\n\n\n# Preprocess function with labels\ndef preprocess_function(examples):\n    inputs = tokenizer(examples[\"review_body\"], truncation=True)\n    inputs[\"labels\"] = examples[\"stars\"]\n    return inputs\n\n\ndef train_model(\n    model_name,\n    num_of_labels,\n    dataset,\n    tokenizer,\n    path_to_save,\n    epochs=1,\n    disable_tqdm=False,\n    batch_size=8,\n):\n    training_args = TrainingArguments(\n        output_dir=path_to_save,\n        warmup_steps=10000,\n        optim=\"adamw_torch\",\n        num_train_epochs=epochs,\n        weight_decay=1e-4,\n        evaluation_strategy=IntervalStrategy.EPOCH,\n        save_strategy=IntervalStrategy.EPOCH,\n        metric_for_best_model=\"f1\",\n        save_total_limit=1,\n        disable_tqdm=disable_tqdm,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n    )\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name, num_labels=num_of_labels\n    )\n    tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True)\n    tokenized_validation = dataset[\"validation\"].map(preprocess_function, batched=True)\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_validation,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    return trainer\n\n\ndef prepare_dataset(train_df, test_df, valid_df, translated_df):\n    train_ds = Dataset.from_pandas(train_df)\n    test_ds = Dataset.from_pandas(test_df)\n    valid_ds = Dataset.from_pandas(valid_df)\n    translated_ds = Dataset.from_pandas(translated_df)\n\n    dataset = DatasetDict()\n\n    dataset[\"train\"] = train_ds\n    dataset[\"validation\"] = valid_ds\n    dataset[\"test\"] = test_ds\n    dataset[\"translated\"] = translated_ds\n\n    return dataset\n\n\ndef run_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    verbose=False,\n    epochs=1,\n    disable_tqdm=True,\n    batch_size=8):\n\n    train, test, val = prepare_data(\n        train_df, test_df, valid_df, columns_to_drop, stars, languages\n    )\n    train = reduce_dataset(train, stars, languages, rows_to_drop, verbose)\n    train = replace_mapping(train, label, mapping)\n    test = replace_mapping(test, label, mapping)\n    val = replace_mapping(val, label, mapping)\n\n    translated = drop_data(translated_test_df, stars, [\"de\", \"es\", \"fr\"], [])\n    translated = replace_mapping(translated, label, mapping)\n\n    global tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, num_labels=num_of_labels)\n    train = prepare_truncation(train, tokenizer, M, N)\n    test = prepare_truncation(test, tokenizer, M, N)\n    val = prepare_truncation(val, tokenizer, M, N)\n    translated = prepare_truncation(translated, tokenizer, M, N)\n\n    dataset = prepare_dataset(train, test, val, translated)\n    trainer = train_model(\n        model_name,\n        num_of_labels,\n        dataset,\n        tokenizer,\n        f\"./{model_name}_labels_{num_of_labels}_M_{M}_N_{N}\",\n        epochs=epochs,\n        disable_tqdm=disable_tqdm,\n        batch_size=batch_size,\n    )\n\n    print(f\"Results for model: {model_name}, M = {M}, N = {N}\")\n    print(\"English test set results\")\n    tokenized_train = dataset[\"test\"].map(preprocess_function, batched=True)\n    eval_model(trainer, tokenized_train, label_names, label)\n    print()\n    print(\"Translated to English set results\")\n    tokenized_train = dataset[\"translated\"].map(preprocess_function, batched=True)\n    eval_model(trainer, tokenized_train, label_names, label)\n    del trainer\n    torch.cuda.empty_cache()","metadata":{"id":"ti8lptcdmLJC","execution":{"iopub.status.busy":"2023-08-10T12:22:11.323613Z","iopub.execute_input":"2023-08-10T12:22:11.324138Z","iopub.status.idle":"2023-08-10T12:22:12.262019Z","shell.execute_reply.started":"2023-08-10T12:22:11.324102Z","shell.execute_reply":"2023-08-10T12:22:12.260967Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"columns_to_drop = ['review_id', 'product_id', 'reviewer_id', 'product_category']\ntokenizer = None\nrows_to_drop = 30000","metadata":{"id":"YKnwdYzLmLJD","execution":{"iopub.status.busy":"2023-08-10T12:22:12.265345Z","iopub.execute_input":"2023-08-10T12:22:12.266345Z","iopub.status.idle":"2023-08-10T12:22:12.271626Z","shell.execute_reply.started":"2023-08-10T12:22:12.266309Z","shell.execute_reply":"2023-08-10T12:22:12.270465Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"stars = [1, 2, 3, 4, 5]\nlanguages = [\"en\"]\nmapping = {1: 0, 2: 0, 3: 0, 4: 1, 5: 1}\nnum_of_labels = 2\nlabel = \"stars\"\nlabel_names = [\"Negative\", \"Positive\"]\nepochs = 1","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:22:12.273169Z","iopub.execute_input":"2023-08-10T12:22:12.273613Z","iopub.status.idle":"2023-08-10T12:22:12.283612Z","shell.execute_reply.started":"2023-08-10T12:22:12.273578Z","shell.execute_reply":"2023-08-10T12:22:12.282539Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"**BERT UNCASED**","metadata":{}},{"cell_type":"markdown","source":"**Two labels with Neutral; N=128, M=382; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"model_name = \"bert-base-uncased\"","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:22:12.285220Z","iopub.execute_input":"2023-08-10T12:22:12.285663Z","iopub.status.idle":"2023-08-10T12:22:12.295439Z","shell.execute_reply.started":"2023-08-10T12:22:12.285620Z","shell.execute_reply":"2023-08-10T12:22:12.294574Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"N = 128\nM = 382\n\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:22:12.298778Z","iopub.execute_input":"2023-08-10T12:22:12.299038Z","iopub.status.idle":"2023-08-10T12:36:17.592714Z","shell.execute_reply.started":"2023-08-10T12:22:12.299015Z","shell.execute_reply":"2023-08-10T12:36:17.591555Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 50000/50000 [00:15<00:00, 3284.59it/s]\nProcessing reviews: 100%|██████████| 5000/5000 [00:01<00:00, 2945.65it/s]\nProcessing reviews: 100%|██████████| 5000/5000 [00:01<00:00, 3395.53it/s]\nProcessing reviews: 100%|██████████| 15000/15000 [00:04<00:00, 3547.66it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6250/6250 12:12, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.339300</td>\n      <td>0.362679</td>\n      <td>0.868600</td>\n      <td>0.869626</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-uncased, M = 382, N = 128\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9232 |   0.8497 |     0.8849 |      3000 |\n| Positive |      0.7986 |   0.8940 |     0.8436 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9253 |   0.8194 |     0.8692 |      9000 |\n| Positive |      0.7688 |   0.9008 |     0.8296 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels with Neutral; N=64, M=64; overall 128 tokens**","metadata":{}},{"cell_type":"code","source":"N = 64\nM = 64\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:36:17.594742Z","iopub.execute_input":"2023-08-10T12:36:17.595362Z","iopub.status.idle":"2023-08-10T12:49:06.585179Z","shell.execute_reply.started":"2023-08-10T12:36:17.595324Z","shell.execute_reply":"2023-08-10T12:49:06.584004Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 50000/50000 [00:16<00:00, 2946.41it/s]\nProcessing reviews: 100%|██████████| 5000/5000 [00:01<00:00, 3094.79it/s]\nProcessing reviews: 100%|██████████| 5000/5000 [00:01<00:00, 2968.26it/s]\nProcessing reviews: 100%|██████████| 15000/15000 [00:05<00:00, 2952.16it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6250/6250 10:59, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.339900</td>\n      <td>0.363241</td>\n      <td>0.871200</td>\n      <td>0.871688</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-uncased, M = 64, N = 64\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9035 |   0.8610 |     0.8817 |      3000 |\n| Positive |      0.8052 |   0.8620 |     0.8326 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9121 |   0.8322 |     0.8703 |      9000 |\n| Positive |      0.7775 |   0.8797 |     0.8255 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels with Neutral; N=0, M=510; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"N = 0\nM = 510\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T12:49:06.588752Z","iopub.execute_input":"2023-08-10T12:49:06.589056Z","iopub.status.idle":"2023-08-10T13:03:16.713418Z","shell.execute_reply.started":"2023-08-10T12:49:06.589029Z","shell.execute_reply":"2023-08-10T13:03:16.712449Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 50000/50000 [00:16<00:00, 3050.54it/s]\nProcessing reviews: 100%|██████████| 5000/5000 [00:01<00:00, 3223.67it/s]\nProcessing reviews: 100%|██████████| 5000/5000 [00:01<00:00, 3177.10it/s]\nProcessing reviews: 100%|██████████| 15000/15000 [00:04<00:00, 3277.94it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6250/6250 12:16, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.341600</td>\n      <td>0.403775</td>\n      <td>0.857400</td>\n      <td>0.858557</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-uncased, M = 510, N = 0\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9171 |   0.8333 |     0.8732 |      3000 |\n| Positive |      0.7801 |   0.8870 |     0.8301 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9287 |   0.7927 |     0.8553 |      9000 |\n| Positive |      0.7450 |   0.9087 |     0.8187 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels with Neutral; N=510, M=0; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"N = 510\nM = 0\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:03:16.715415Z","iopub.execute_input":"2023-08-10T13:03:16.715798Z","iopub.status.idle":"2023-08-10T13:17:28.892140Z","shell.execute_reply.started":"2023-08-10T13:03:16.715762Z","shell.execute_reply":"2023-08-10T13:17:28.891138Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 50000/50000 [00:16<00:00, 2969.62it/s]\nProcessing reviews: 100%|██████████| 5000/5000 [00:01<00:00, 3115.46it/s]\nProcessing reviews: 100%|██████████| 5000/5000 [00:01<00:00, 3094.02it/s]\nProcessing reviews: 100%|██████████| 15000/15000 [00:04<00:00, 3269.51it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6250/6250 12:17, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.341400</td>\n      <td>0.366499</td>\n      <td>0.865800</td>\n      <td>0.866665</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-uncased, M = 0, N = 510\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9134 |   0.8547 |     0.8831 |      3000 |\n| Positive |      0.8012 |   0.8785 |     0.8381 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9236 |   0.8061 |     0.8609 |      9000 |\n| Positive |      0.7558 |   0.9000 |     0.8216 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**BERT CASED**","metadata":{}},{"cell_type":"code","source":"stars = [1, 2, 4, 5]\nlanguages = [\"en\"]\nmapping = {1: 0, 2: 0, 4: 1, 5: 1}\nnum_of_labels = 2\nlabel = \"stars\"\nlabel_names = [\"Negative\", \"Positive\"]\nepochs = 1","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:17:28.894204Z","iopub.execute_input":"2023-08-10T13:17:28.894794Z","iopub.status.idle":"2023-08-10T13:17:28.901118Z","shell.execute_reply.started":"2023-08-10T13:17:28.894755Z","shell.execute_reply":"2023-08-10T13:17:28.899887Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=128, M=382; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"model_name = \"bert-base-cased\"\nN = 128\nM = 382\n\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:17:28.902559Z","iopub.execute_input":"2023-08-10T13:17:28.903289Z","iopub.status.idle":"2023-08-10T13:28:50.881965Z","shell.execute_reply.started":"2023-08-10T13:17:28.903255Z","shell.execute_reply":"2023-08-10T13:28:50.881009Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"178e95e180ad411aba0ee821f8fcfc7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3874e259b39a425e8a04953c7af4d7ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82751be08bc64873ac4fb943f928b7f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960207f65a2e4b32a0eadabf11ee92a0"}},"metadata":{}},{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:11<00:00, 3427.03it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2684.83it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3121.66it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:03<00:00, 3574.70it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"530297d955a443fc968dc9c8e4ae9256"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 09:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.279500</td>\n      <td>0.297899</td>\n      <td>0.913750</td>\n      <td>0.913715</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-cased, M = 382, N = 128\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9031 |   0.9365 |     0.9195 |      2000 |\n| Positive |      0.9341 |   0.8995 |     0.9165 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9054 |   0.9170 |     0.9112 |      6000 |\n| Positive |      0.9159 |   0.9042 |     0.9100 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=64, M=64; overall 128 tokens**","metadata":{}},{"cell_type":"code","source":"N = 64\nM = 64\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:28:50.883833Z","iopub.execute_input":"2023-08-10T13:28:50.884411Z","iopub.status.idle":"2023-08-10T13:39:08.409990Z","shell.execute_reply.started":"2023-08-10T13:28:50.884375Z","shell.execute_reply":"2023-08-10T13:39:08.408949Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:12<00:00, 3181.15it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3382.42it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3260.24it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:03<00:00, 3528.04it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 08:52, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.289000</td>\n      <td>0.331093</td>\n      <td>0.910250</td>\n      <td>0.910248</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-cased, M = 64, N = 64\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9006 |   0.9240 |     0.9121 |      2000 |\n| Positive |      0.9220 |   0.8980 |     0.9098 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9165 |   0.8928 |     0.9045 |      6000 |\n| Positive |      0.8955 |   0.9187 |     0.9070 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=510, M=0; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"N = 510\nM = 0\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:39:08.412693Z","iopub.execute_input":"2023-08-10T13:39:08.413072Z","iopub.status.idle":"2023-08-10T13:50:27.705254Z","shell.execute_reply.started":"2023-08-10T13:39:08.413035Z","shell.execute_reply":"2023-08-10T13:50:27.704079Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:11<00:00, 3382.29it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2927.47it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3432.47it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:03<00:00, 3634.54it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 09:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.288900</td>\n      <td>0.283645</td>\n      <td>0.914000</td>\n      <td>0.913958</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-cased, M = 0, N = 510\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9012 |   0.9345 |     0.9175 |      2000 |\n| Positive |      0.9320 |   0.8975 |     0.9144 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9091 |   0.9105 |     0.9098 |      6000 |\n| Positive |      0.9104 |   0.9090 |     0.9097 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=0, M=510; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"N = 0\nM = 510\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T13:50:27.711161Z","iopub.execute_input":"2023-08-10T13:50:27.711462Z","iopub.status.idle":"2023-08-10T14:01:48.691438Z","shell.execute_reply.started":"2023-08-10T13:50:27.711436Z","shell.execute_reply":"2023-08-10T14:01:48.690383Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:11<00:00, 3368.02it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3438.50it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3249.93it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:03<00:00, 3614.45it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 09:51, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.292300</td>\n      <td>0.380517</td>\n      <td>0.901750</td>\n      <td>0.901486</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-cased, M = 510, N = 0\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.8654 |   0.9615 |     0.9109 |      2000 |\n| Positive |      0.9567 |   0.8505 |     0.9005 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.8657 |   0.9475 |     0.9048 |      6000 |\n| Positive |      0.9420 |   0.8530 |     0.8953 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"BERT UNCASED","metadata":{}},{"cell_type":"markdown","source":"**Two labels without Neutral; N=128, M=382; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"model_name = \"bert-base-uncased\"","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:01:48.693383Z","iopub.execute_input":"2023-08-10T14:01:48.693760Z","iopub.status.idle":"2023-08-10T14:01:48.698261Z","shell.execute_reply.started":"2023-08-10T14:01:48.693723Z","shell.execute_reply":"2023-08-10T14:01:48.697190Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"N = 128\nM = 382\n\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:01:48.699896Z","iopub.execute_input":"2023-08-10T14:01:48.700543Z","iopub.status.idle":"2023-08-10T14:13:01.968160Z","shell.execute_reply.started":"2023-08-10T14:01:48.700508Z","shell.execute_reply":"2023-08-10T14:13:01.967142Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:13<00:00, 3046.25it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3242.18it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3171.36it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:03<00:00, 3334.14it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 09:42, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.267000</td>\n      <td>0.314819</td>\n      <td>0.909000</td>\n      <td>0.908977</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-uncased, M = 382, N = 128\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9221 |   0.9115 |     0.9168 |      2000 |\n| Positive |      0.9125 |   0.9230 |     0.9177 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9290 |   0.8877 |     0.9079 |      6000 |\n| Positive |      0.8925 |   0.9322 |     0.9119 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=64, M=64; overall 128 tokens**","metadata":{}},{"cell_type":"code","source":"N = 64\nM = 64\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:13:01.970050Z","iopub.execute_input":"2023-08-10T14:13:01.970550Z","iopub.status.idle":"2023-08-10T14:23:15.790335Z","shell.execute_reply.started":"2023-08-10T14:13:01.970511Z","shell.execute_reply":"2023-08-10T14:23:15.789220Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:13<00:00, 2941.51it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3077.58it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2903.50it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:03<00:00, 3250.62it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 08:47, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.264900</td>\n      <td>0.306264</td>\n      <td>0.918000</td>\n      <td>0.917992</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-uncased, M = 64, N = 64\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9244 |   0.9175 |     0.9210 |      2000 |\n| Positive |      0.9181 |   0.9250 |     0.9215 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9259 |   0.8973 |     0.9114 |      6000 |\n| Positive |      0.9004 |   0.9282 |     0.9141 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=510, M=0; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"N = 510\nM = 0\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:23:15.792477Z","iopub.execute_input":"2023-08-10T14:23:15.792870Z","iopub.status.idle":"2023-08-10T14:34:28.633676Z","shell.execute_reply.started":"2023-08-10T14:23:15.792833Z","shell.execute_reply":"2023-08-10T14:34:28.632535Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:12<00:00, 3081.13it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3135.16it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2857.34it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:04<00:00, 2849.56it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 09:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.262200</td>\n      <td>0.291302</td>\n      <td>0.912500</td>\n      <td>0.912495</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-uncased, M = 0, N = 510\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9206 |   0.9165 |     0.9186 |      2000 |\n| Positive |      0.9169 |   0.9210 |     0.9189 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9269 |   0.8817 |     0.9037 |      6000 |\n| Positive |      0.8872 |   0.9305 |     0.9083 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=0, M=510; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"N = 0\nM = 510\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:34:28.635516Z","iopub.execute_input":"2023-08-10T14:34:28.635872Z","iopub.status.idle":"2023-08-10T14:45:42.105819Z","shell.execute_reply.started":"2023-08-10T14:34:28.635837Z","shell.execute_reply":"2023-08-10T14:45:42.104762Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:13<00:00, 3042.73it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3250.50it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 3154.68it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:03<00:00, 3270.20it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 09:42, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.263400</td>\n      <td>0.312010</td>\n      <td>0.911000</td>\n      <td>0.910933</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: bert-base-uncased, M = 510, N = 0\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9376 |   0.8870 |     0.9116 |      2000 |\n| Positive |      0.8928 |   0.9410 |     0.9163 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9430 |   0.8550 |     0.8969 |      6000 |\n| Positive |      0.8674 |   0.9483 |     0.9061 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**XLNET 2 labels without Neutrals**","metadata":{}},{"cell_type":"code","source":"model_name = \"xlnet-base-cased\"","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:45:42.108110Z","iopub.execute_input":"2023-08-10T14:45:42.108525Z","iopub.status.idle":"2023-08-10T14:45:42.114631Z","shell.execute_reply.started":"2023-08-10T14:45:42.108472Z","shell.execute_reply":"2023-08-10T14:45:42.112071Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=128, M=382; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"N = 128\nM = 382\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:45:42.116458Z","iopub.execute_input":"2023-08-10T14:45:42.117318Z","iopub.status.idle":"2023-08-10T15:00:49.931461Z","shell.execute_reply.started":"2023-08-10T14:45:42.117282Z","shell.execute_reply":"2023-08-10T15:00:49.930529Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"446be4411dc84789a2e2210781970450"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0faf12ee00ef46d08a6bb616ac2bf890"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88f244d5459747c2aeeaea152d8b191f"}},"metadata":{}},{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:13<00:00, 2903.25it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2795.88it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2831.58it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:03<00:00, 3066.93it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fcfebdb9a7c457086f730e6b55dc710"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 13:04, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.294700</td>\n      <td>0.338842</td>\n      <td>0.919250</td>\n      <td>0.919246</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: xlnet-base-cased, M = 382, N = 128\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9067 |   0.9325 |     0.9194 |      2000 |\n| Positive |      0.9305 |   0.9040 |     0.9171 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9228 |   0.9058 |     0.9142 |      6000 |\n| Positive |      0.9075 |   0.9242 |     0.9158 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=64, M=64; overall 128 tokens**","metadata":{}},{"cell_type":"code","source":"N = 64\nM = 64\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:00:49.933372Z","iopub.execute_input":"2023-08-10T15:00:49.933777Z","iopub.status.idle":"2023-08-10T15:13:51.599070Z","shell.execute_reply.started":"2023-08-10T15:00:49.933741Z","shell.execute_reply":"2023-08-10T15:13:51.597951Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:14<00:00, 2681.99it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2808.38it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2710.67it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:04<00:00, 2999.69it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 11:12, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.311700</td>\n      <td>0.384186</td>\n      <td>0.918250</td>\n      <td>0.918249</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: xlnet-base-cased, M = 64, N = 64\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9158 |   0.9305 |     0.9231 |      2000 |\n| Positive |      0.9294 |   0.9145 |     0.9219 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9271 |   0.9152 |     0.9211 |      6000 |\n| Positive |      0.9162 |   0.9280 |     0.9221 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=510, M=0; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"N = 510\nM = 0\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:13:51.601050Z","iopub.execute_input":"2023-08-10T15:13:51.601651Z","iopub.status.idle":"2023-08-10T15:28:54.622363Z","shell.execute_reply.started":"2023-08-10T15:13:51.601612Z","shell.execute_reply":"2023-08-10T15:28:54.621332Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:14<00:00, 2735.34it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2804.00it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2889.25it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:03<00:00, 3044.02it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 13:02, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.288400</td>\n      <td>0.346373</td>\n      <td>0.920250</td>\n      <td>0.920186</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Results for model: xlnet-base-cased, M = 0, N = 510\nEnglish test set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.8854 |   0.9615 |     0.9219 |      2000 |\n| Positive |      0.9579 |   0.8755 |     0.9148 |      2000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n\nTranslated to English set results\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"+----------+-------------+----------+------------+-----------+\n| Class    |   Precision |   Recall |   F1-Score |   Support |\n|----------+-------------+----------+------------+-----------|\n| Negative |      0.9026 |   0.9328 |     0.9175 |      6000 |\n| Positive |      0.9305 |   0.8993 |     0.9147 |      6000 |\n+----------+-------------+----------+------------+-----------+\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Two labels without Neutral; N=0, M=510; overall 510 tokens**","metadata":{}},{"cell_type":"code","source":"N = 0\nM = 510\n\nrun_and_eval(\n    train_df,\n    test_df,\n    valid_df,\n    stars,\n    languages,\n    columns_to_drop,\n    rows_to_drop,\n    label,\n    mapping,\n    model_name,\n    num_of_labels,\n    M,\n    N,\n    label_names,\n    False,\n    epochs,\n    False,\n    batch_size=2\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T16:24:32.259589Z","iopub.execute_input":"2023-08-10T16:24:32.260001Z","iopub.status.idle":"2023-08-10T16:25:04.973224Z","shell.execute_reply.started":"2023-08-10T16:24:32.259971Z","shell.execute_reply":"2023-08-10T16:25:04.971743Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"Processing reviews: 100%|██████████| 40000/40000 [00:13<00:00, 2985.80it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2971.32it/s]\nProcessing reviews: 100%|██████████| 4000/4000 [00:01<00:00, 2844.94it/s]\nProcessing reviews: 100%|██████████| 12000/12000 [00:04<00:00, 2765.89it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m4\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0mN = \u001b[94m0\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0mM = \u001b[94m510\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 4 run_and_eval(                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0mtrain_df,                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0mtest_df,                                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0mvalid_df,                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92mrun_and_eval\u001b[0m:\u001b[94m223\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m│   \u001b[0mtranslated = prepare_truncation(translated, tokenizer, M, N)                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   \u001b[0mdataset = prepare_dataset(train, test, val, translated)                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m223 \u001b[2m│   \u001b[0mtrainer = train_model(                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m│   │   \u001b[0mmodel_name,                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m│   │   \u001b[0mnum_of_labels,                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m│   │   \u001b[0mdataset,                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92mtrain_model\u001b[0m:\u001b[94m164\u001b[0m                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m161 \u001b[0m\u001b[2m│   │   \u001b[0mcompute_metrics=compute_metrics,                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m164 \u001b[2m│   \u001b[0mtrainer.train()                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m trainer                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1645\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1642 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1643 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1644 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1645 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1646 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1647 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1648 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2007\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2004 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mscale_after = \u001b[96mself\u001b[0m.scaler.get_scale()                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2005 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0moptimizer_was_run = scale_before <= scale_after                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2006 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2007 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.optimizer.step()                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2008 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0moptimizer_was_run = \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m.accelerator.optimizer_step_was_skip  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2009 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2010 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m optimizer_was_run:                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33moptimizer.py\u001b[0m:\u001b[94m140\u001b[0m in \u001b[92mstep\u001b[0m                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# If we reduced the loss scale, it means the optimizer step was skipped \u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._is_overflow = scale_after < scale_before                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m140 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.optimizer.step(closure)                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m142 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_switch_parameters\u001b[0m(\u001b[96mself\u001b[0m, parameters_map):                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m143 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m param_group \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.optimizer.param_groups:                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/optim/\u001b[0m\u001b[1;33mlr_scheduler.py\u001b[0m:\u001b[94m69\u001b[0m in \u001b[92mwrapper\u001b[0m                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  66 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minstance = instance_ref()                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  67 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minstance._step_count += \u001b[94m1\u001b[0m                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  68 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mwrapped = func.\u001b[92m__get__\u001b[0m(instance, \u001b[96mcls\u001b[0m)                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m  69 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapped(*args, **kwargs)                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  70 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  71 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Note that the returned function here is no longer a bound method,\u001b[0m           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  72 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# so attributes like `__func__` and `__self__` no longer exist.\u001b[0m               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/optim/\u001b[0m\u001b[1;33moptimizer.py\u001b[0m:\u001b[94m280\u001b[0m in \u001b[92mwrapper\u001b[0m                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mfunc\u001b[33m}\u001b[0m\u001b[33m must return None or a tuple of (\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │      \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mbut got \u001b[0m\u001b[33m{\u001b[0mresult\u001b[33m}\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\"\u001b[0m)                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m279 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m280 \u001b[2m│   │   │   │   \u001b[0mout = func(*args, **kwargs)                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m281 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._optimizer_step_code()                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m283 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# call optimizer step post hooks\u001b[0m                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/optim/\u001b[0m\u001b[1;33moptimizer.py\u001b[0m:\u001b[94m33\u001b[0m in \u001b[92m_use_grad\u001b[0m                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 30 \u001b[0m\u001b[2m│   │   \u001b[0mprev_grad = torch.is_grad_enabled()                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 31 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 32 \u001b[0m\u001b[2m│   │   │   \u001b[0mtorch.set_grad_enabled(\u001b[96mself\u001b[0m.defaults[\u001b[33m'\u001b[0m\u001b[33mdifferentiable\u001b[0m\u001b[33m'\u001b[0m])                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 33 \u001b[2m│   │   │   \u001b[0mret = func(\u001b[96mself\u001b[0m, *args, **kwargs)                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 35 \u001b[0m\u001b[2m│   │   │   \u001b[0mtorch.set_grad_enabled(prev_grad)                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 36 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m ret                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/optim/\u001b[0m\u001b[1;33madamw.py\u001b[0m:\u001b[94m171\u001b[0m in \u001b[92mstep\u001b[0m                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mstate_steps,                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m169 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m171 \u001b[2m│   │   │   \u001b[0madamw(                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mparams_with_grad,                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mgrads,                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mexp_avgs,                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/optim/\u001b[0m\u001b[1;33madamw.py\u001b[0m:\u001b[94m321\u001b[0m in \u001b[92madamw\u001b[0m                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m318 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m319 \u001b[0m\u001b[2m│   │   \u001b[0mfunc = _single_tensor_adamw                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m320 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m321 \u001b[2m│   \u001b[0mfunc(                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m322 \u001b[0m\u001b[2m│   │   \u001b[0mparams,                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m323 \u001b[0m\u001b[2m│   │   \u001b[0mgrads,                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m324 \u001b[0m\u001b[2m│   │   \u001b[0mexp_avgs,                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/optim/\u001b[0m\u001b[1;33madamw.py\u001b[0m:\u001b[94m564\u001b[0m in \u001b[92m_multi_tensor_adamw\u001b[0m          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m561 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtorch._foreach_div_(max_exp_avg_sq_sqrt, bias_correction2_sqrt)            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m562 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdenom = torch._foreach_add(max_exp_avg_sq_sqrt, eps)                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m563 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m564 \u001b[2m│   │   │   │   \u001b[0mexp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m565 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtorch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m566 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdenom = torch._foreach_add(exp_avg_sq_sqrt, eps)                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m567 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m20.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m15.90\u001b[0m GiB total capacity; \u001b[1;36m14.68\u001b[0m GiB \nalready allocated; \u001b[1;36m13.75\u001b[0m MiB free; \u001b[1;36m15.00\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated \nmemory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>N = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>M = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">510</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 4 run_and_eval(                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>train_df,                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>test_df,                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>valid_df,                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_and_eval</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">223</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>translated = prepare_truncation(translated, tokenizer, M, N)                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">221 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">222 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>dataset = prepare_dataset(train, test, val, translated)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>223 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>trainer = train_model(                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">224 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model_name,                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">225 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>num_of_labels,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">226 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>dataset,                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_model</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">164</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">161 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>compute_metrics=compute_metrics,                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>)                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>164 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>trainer.train()                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> trainer                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1645</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1642 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1643 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1644 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1645 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1646 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1647 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1648 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2007</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2004 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>scale_after = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.get_scale()                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2005 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>optimizer_was_run = scale_before &lt;= scale_after                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2006 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2007 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer.step()                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2008 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>optimizer_was_run = <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.optimizer_step_was_skip  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2009 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2010 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> optimizer_was_run:                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimizer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">140</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># If we reduced the loss scale, it means the optimizer step was skipped </span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">138 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._is_overflow = scale_after &lt; scale_before                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>140 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer.step(closure)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">142 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_switch_parameters</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, parameters_map):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">143 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> param_group <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer.param_groups:                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">lr_scheduler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">69</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  66 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>instance = instance_ref()                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  67 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>instance._step_count += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  68 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>wrapped = func.<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__get__</span>(instance, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>  69 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapped(*args, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  70 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  71 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Note that the returned function here is no longer a bound method,</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  72 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># so attributes like `__func__` and `__self__` no longer exist.</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimizer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">280</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>func<span style=\"color: #808000; text-decoration-color: #808000\">} must return None or a tuple of (</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   │      </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"but got {</span>result<span style=\"color: #808000; text-decoration-color: #808000\">}.\"</span>)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>280 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>out = func(*args, **kwargs)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">281 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._optimizer_step_code()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">283 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># call optimizer step post hooks</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimizer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">33</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_use_grad</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>prev_grad = torch.is_grad_enabled()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>torch.set_grad_enabled(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.defaults[<span style=\"color: #808000; text-decoration-color: #808000\">'differentiable'</span>])                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 33 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>ret = func(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>torch.set_grad_enabled(prev_grad)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> ret                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">adamw.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">171</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>state_steps,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">169 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>171 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>adamw(                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>params_with_grad,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">173 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>grads,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>exp_avgs,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">adamw.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">321</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">adamw</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">318 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>func = _single_tensor_adamw                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">320 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>321 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>func(                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">322 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>params,                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>grads,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">324 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>exp_avgs,                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">adamw.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">564</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_multi_tensor_adamw</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">561 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>torch._foreach_div_(max_exp_avg_sq_sqrt, bias_correction2_sqrt)            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">562 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>denom = torch._foreach_add(max_exp_avg_sq_sqrt, eps)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">563 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>564 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">565 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">566 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>denom = torch._foreach_add(exp_avg_sq_sqrt, eps)                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">567 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.90</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.68</span> GiB \nalready allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.75</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.00</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated \nmemory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}